{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "* [Repositorio de Pandas](https://github.com/pandas-dev/pandas)\n",
    "* [Documentación Oficial](https://pandas.pydata.org/)\n",
    "\n",
    "***\n",
    "## DataFrame (pd.DataFrame)\n",
    "\n",
    "[Documentación de pd.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame)\n",
    "\n",
    "**Pandas DataFrame** es una **estructura de datos bidimensional y etiquetada**, diseñada para manejar y analizar datos tabulares en el entorno de programación Python. Su desarrollo se basa en la biblioteca NumPy y proporciona funcionalidades adicionales para facilitar la manipulación y análisis de datos.\n",
    "\n",
    "#### Características Principales:\n",
    "\n",
    "* **Bidimensionalidad**: Un DataFrame consiste en filas y columnas, organizando los datos en una tabla rectangular. Cada columna puede contener diferentes tipos de datos, permitiendo la representación de información heterogénea.\n",
    "\n",
    "* **Etiquetado**: Tanto las filas como las columnas están etiquetadas, lo que facilita el acceso y la manipulación de datos. Los índices y nombres de columnas pueden ser personalizados para reflejar la naturaleza específica de los datos.\n",
    "\n",
    "* **Integración con NumPy**: Se basa en la eficiente biblioteca NumPy, permitiendo operaciones vectorizadas y aprovechando las ventajas de la computación numérica en Python.\n",
    "\n",
    "#### Ventajas de Pandas DataFrame:\n",
    "\n",
    "* **Facilidad de Uso**: Proporciona una interfaz intuitiva y fácil de usar para realizar operaciones complejas en datos tabulares.\n",
    "\n",
    "* **Manejo de Datos Faltantes**: Ofrece herramientas para manejar valores nulos o faltantes de manera eficiente, evitando la pérdida de información durante el análisis.\n",
    "\n",
    "* **Operaciones de Series Temporales**: Incorpora funcionalidades avanzadas para el trabajo con datos de series temporales, facilitando el análisis de tendencias a lo largo del tiempo.\n",
    "\n",
    "* **Entrada y Salida de Datos**: Permite la lectura y escritura de datos en varios formatos, como CSV, Excel, SQL, y más, favoreciendo la interoperabilidad con otras herramientas y sistemas.\n",
    "\n",
    "* **Operaciones de Agrupación y Agregación**: Facilita la agrupación de datos basada en criterios específicos y la aplicación de funciones de agregación para resumir la información.\n",
    "\n",
    "#### Funcionalidades Clave:\n",
    "\n",
    "* **Indexación y Selección Eficiente**: Permite el acceso y manipulación de datos utilizando etiquetas de filas y columnas, así como índices booleanos.\n",
    "\n",
    "* **Operaciones Estadísticas y Matemáticas**: Proporciona métodos integrados para realizar cálculos estadísticos, operaciones matemáticas y transformaciones en los datos.\n",
    "\n",
    "* **Visualización Integrada**: Se integra con bibliotecas de visualización como Matplotlib y Seaborn, facilitando la creación de gráficos y visualizaciones de datos.\n",
    "\n",
    "* **Manipulación de Datos Complejos**: Ofrece funcionalidades para la limpieza, filtración, combinación y transformación de datos complejos, lo que facilita la preparación de datos para el análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agenda**\n",
    "\n",
    "* Creación de pd.DataFrame\n",
    "* Analogía/relación entre pd.DataFrame y pd.Series\n",
    "* Características\n",
    "* Operaciones vectoriales\n",
    "* Operaciones matemáticas\n",
    "* Operaciones estadísticas\n",
    "* Operaciones lógicas\n",
    "* Filtrado y slicing\n",
    "* Interacción con NumPy\n",
    "* Concatenar y Merges\n",
    "* Relleno de valores faltantes\n",
    "* Agrupación y operaciones agrupadas\n",
    "* Leer y guardar pd.DataFrame desde csv, parquet y url."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Crear un pd.DataFrame\n",
    "\n",
    "Existen diversas formas de generar un pd.DataFrame a partir de las siguientes estructuras:\n",
    "1. *dict*\n",
    "1. *pd.Series* (o *list*)\n",
    "1. *np.ndarray*\n",
    "\n",
    "En todas las alternativas se debe usar el constructor de *Pandas*: ```pd.DataFrame(...)```\n",
    "\n",
    "*Existen otras maneras adicionales de hacerlo, pero para el alcance de este curso las que se presentan anteriormente son suficientes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar la librería\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. dict\n",
    "\n",
    "# definir un dict\n",
    "d = {'columna_1': [0,1,2,3,4,5,6,7,8,9], 'columna_2': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]}\n",
    "\n",
    "# por convensión se lo llama \"df\"\n",
    "df = pd.DataFrame(data = d)\n",
    "\n",
    "# \"display\" permite ver el datafgrame (es otra alternativa)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('df es un pd.DataFrame:', type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. pd.Series o lists\n",
    "\n",
    "serie_1 = pd.Series([i**2 for i in range(10)], name='valores_cuadrados')\n",
    "serie_2 = pd.Series([i + 5 for i in range(10)], name= 'valores_mas_5')\n",
    "\n",
    "# usar la función pd.concat (vista en el notebook anterior)\n",
    "df = pd.concat([serie_1, serie_2],axis=1)\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_1 = [i**2 for i in range(10)]\n",
    "lista_2 = [i + 5 for i in range(10)]\n",
    "\n",
    "col_names = ['valores_cuadrados', 'valores_mas_5']\n",
    "\n",
    "# usar la función pd.concat (vista en el notebook anterior)\n",
    "df = pd.concat([serie_1, serie_2],axis=1)\n",
    "\n",
    "# cambiar los nombres ya que las listas NO tienen nombres\n",
    "df.columns = col_names\n",
    "\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. np.ndarray\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# crea el array y camniar el shape a una sola columna\n",
    "array_1d = np.array([range(0, 20, 2)]).reshape((-1,))\n",
    "\n",
    "df = pd.DataFrame(array_1d, columns=['columna 1'])\n",
    "\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_nd = np.random.random(size=(20, 3))\n",
    "\n",
    "df = pd.DataFrame(array_nd, columns=[f'columna {i}' for i in range(array_nd.shape[1])])\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogía/relación entre pd.DataFrame y pd.Series\n",
    "\n",
    "Es importante resaltar que existe una clara relación entre ```pd.DataFrame``` y ```pd.Series```: un ```pd.DataFrame``` es **un conjunto de** ```pd.Series``` **concatenadas de forma horizontal**.\n",
    "\n",
    "Mediante el uso de *slcing* (visto en el notebook previo y que posteriormente será ampliado en este notebook) se puede demostrar esta afirmación:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'columna_1': [0,1,2,3,4,5,6,7,8,9], 'columna_2': [10,11,12,13,14,15,16,17,18,19]}\n",
    "\n",
    "df = pd.DataFrame(data = d)\n",
    "\n",
    "col_1 = df['columna_1']\n",
    "col_2 = df['columna_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'- df es un {type(df)}', f'col_1 es una {type(col_1)}', f'col_2 es una {type(col_2)}', sep='\\n- ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing\n",
    "\n",
    "Una funcionalidad muy importante de los pd.DataFrame es poder 'aislar' las columnas que lo componen y realizar cálculos con ellas sin afectar el resto de las columnas.\n",
    "\n",
    "Para ello existen tres formas diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'columna_1': range(20), 'columna_2': range(20)}\n",
    "\n",
    "df = pd.DataFrame(data = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en estos 3 casos se extrae la columna seleccionada como una pd.Series\n",
    "\n",
    "# alternativa 1\n",
    "display(df['columna_1'].head())\n",
    "\n",
    "# alternativa 2\n",
    "display(df.loc[:, 'columna_1'].head())\n",
    "\n",
    "# alternativa 3\n",
    "display(df.iloc[:, 0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sin embargo, si se busca extraer dicha columna pero con el tipo de pd.DataFrame se recurre a la siguiente sintaxis\n",
    "\n",
    "# alternativa 1\n",
    "display(df[['columna_1']].head())\n",
    "\n",
    "# alternativa 2\n",
    "display(df.loc[:, ['columna_1']].head())\n",
    "\n",
    "# alternativa 3\n",
    "display(df.iloc[:,[0]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen,\n",
    "```python\n",
    "\n",
    "df['columna_1'] ---> extrae pd.Series\n",
    "\n",
    "df[['columna_1']] ---> extrae pd.DataFrame\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importante: Esto implica que la gran mayoría de las operaciones que funcionan en las pd.Series con aplicables a los pd.DataFrame!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características de un pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_int = np.random.choice(a=[10, 20, 30, 40, 50], size=(40, 6))\n",
    "df_int = pd.DataFrame(array_int, columns=[f'columna {i}' for i in range(array_int.shape[1])], dtype=int)\n",
    "\n",
    "array_float = np.random.random(size=(50, 5))\n",
    "df_float = pd.DataFrame(array_float, columns=[f'columna {i}' for i in range(array_float.shape[1])],dtype=float)\n",
    "\n",
    "array_str = np.random.choice(a= ['a', 'b', 'c'], size=(35, 6))\n",
    "df_str = pd.DataFrame(array_str, columns=[f'columna {i}' for i in range(array_str.shape[1])], dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, df in enumerate([df_int, df_float, df_str]):\n",
    "    print('El df #{} posee {} registros y {} columnas.\\nSus datos son del tipo\\n{}\\n'.format(n+1, df.shape[0], df.shape[1], df.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es posible analizar si el df posee valores nulos y ver en qué columnas se encuentran\n",
    "\n",
    "df_nans = pd.DataFrame(np.random.choice(a=[np.nan, 1, 2, 3 ,4, 5], size=(100, 5)), columns=[f'columna_{i}' for i in range(5)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con el método \"isna()\" se puede visualizar, mediante un dataframe de valores booleanos, si el valor de cierta posición es nulo\n",
    "df_nans.isna().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se pueden sumar dichas columnas de booleanos y obtener la cantidad de nulos en cada columna\n",
    "df_nans.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y volverlas a sumar para saber la cantidad total de nulos en el dataframe\n",
    "df_nans.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se puede ver el nombre de las columnas\n",
    "df_nans.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cantidad de elementos únicos en cada columna y cantidad de cada tipo de elemento\n",
    "for col in df_str.columns:\n",
    "    print('\\n--', col)\n",
    "    print(df_str[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para visualizar el df en forma parcial, se usa el método 'head' o 'tail'\n",
    "\n",
    "display(df.head(5))\n",
    "display(df.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones vectorizadas\n",
    "\n",
    "La vectorización es un concepto clave en el ámbito de la ciencia de datos y la programación computacional que sirve como un elemento fundamental para el manejo eficiente de datos en bibliotecas como Pandas. La idea se refiere fundamentalmente a la capacidad de realizar operaciones en conjuntos completos de datos, en lugar de iterar a través de cada elemento de manera individual.\n",
    "\n",
    "Este enfoque optimizado proporciona un rendimiento más eficiente al procesar grandes conjuntos de datos, ya que aprovecha las operaciones vectoriales implementadas en bibliotecas como NumPy y Pandas. En lugar de ejecutar operaciones en cada elemento de forma secuencial, la vectorización permite realizar cálculos en bloques de datos de manera simultánea, mejorando significativamente la velocidad de ejecución.\n",
    "\n",
    "En el contexto de Pandas, la vectorización permite realizar operaciones aritméticas, de comparación y aplicar funciones universales (ufuncs) a nivel de DataFrame o Serie de manera eficiente. Esta capacidad es crucial para el análisis y la manipulación eficientes de grandes conjuntos de datos, ya que evita la necesidad de bucles explícitos y facilita la escritura de código más conciso y legible. En resumen, la vectorización es una herramienta esencial que contribuye a la eficiencia y capacidad de procesamiento en la manipulación de datos en entornos de ciencia de datos y programación computacional.\n",
    "\n",
    "Ver:\n",
    "* [What Is A Vectorized Operation In Pandas](https://vegibit.com/what-is-a-vectorized-operation-in-pandas/#the-concept-of-vectorization-a-general-overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randint(1, 100, size=100_000)\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df_loop = df.copy()\n",
    "\n",
    "for row in range(df.shape[0]):\n",
    "    df_loop.iloc[row] = df_loop.iloc[row]**2\n",
    "\n",
    "print(\"Proceso por medio de un loop: \", time.time() - start_time, \"segundos\") # con 100_000 datos tarda aprox entre 23 y 30 segundos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_vect = df.copy()\n",
    "df_vect = df_vect**2\n",
    "\n",
    "print(\"Proceso por medio de un loop: \", time.time() - start_time, \"segundos\") # con 100_000 datos tarda aprox 0.0009 segundos...\n",
    "\n",
    "# el loop tardó unas 27.000 veces más que la opoeración vectorial!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df, df_loop, df_vect], axis = 1).sample(10) # se obtienen los mismos resultados en una mínima fracción de tiempo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Dado que se demostró el potencial de la vectorización, todas las operaciones que se realizarán en los pd.DataFrame's se realizarán de forma vectorial. Por suerte, la librería Pandas y NumPy proveen gran cantidad de funciones y métodos que permiten tomar esa ventaja y aplicarla sin ninguna clase de complicación.**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones matemáticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame(np.random.randint(low=0, high=100, size=(100, 5)), columns=[f'columna_{i}' for i in range(5)])\n",
    "df_2 = pd.DataFrame(np.random.randint(low=20, high=150, size=(100, 2)), columns=[f'columna_{i}' for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suma, resta, multiplicación y división\n",
    "\n",
    "print('Suma')\n",
    "suma = df_1['columna_0'] + df_1['columna_1']\n",
    "print(suma.head())\n",
    "\n",
    "print('Resta')\n",
    "resta = df_1['columna_2'] - df_1['columna_3']\n",
    "print(resta.head())\n",
    "\n",
    "print('Multiplicación')\n",
    "mult = df_1['columna_0'] * df_2['columna_0']\n",
    "print(mult.head())\n",
    "\n",
    "print('División')\n",
    "div = df_1['columna_4'] / df_1['columna_2']\n",
    "print(div.head())\n",
    "\n",
    "print('Suma de un escalar')\n",
    "suma_esc = df_2['columna_1'] + 10\n",
    "print(suma_esc.head())\n",
    "\n",
    "print('Producto de un escalar')\n",
    "mult_esc = df_1['columna_0'] * 1.75\n",
    "print(mult_esc.head())\n",
    "\n",
    "print('Potencia')\n",
    "pot = df_1['columna_3'].pow(2) # equivalente a serie_2**2\n",
    "\n",
    "# todas estas operaciones tiene un método de pandas asociado, pero la sintaxis es más compleja y se obtiene el mismo resultado en el mismo tiempo de cómputo\n",
    "# a diferencia de las pd.Series, nos permite además aplicar la operación en cuestión a más de una columna\n",
    "df_1[['columna_0', 'columna_1']] + df_2[['columna_0', 'columna_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a diferencia de las pd.Series, nos permite además aplicar la operación en cuestión a más de una columna\n",
    "suma = df_1[['columna_0', 'columna_1']] + df_2[['columna_0', 'columna_1']]\n",
    "display(suma.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones estadísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas posee un método que permite hacer un primer abordaje descriptivo muy rápido\n",
    "\n",
    "df_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# promedio, desvío y percentiles\n",
    "\n",
    "promedio = df_1.mean()\n",
    "desvio = df_1.std()\n",
    "perc = df_1.quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "print(f'Promedio:\\n{round(promedio, 4)}', f'Desvío:\\n{round(desvio, 4)}', 'Percentiles:', perc, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# también se pueden calcular para una columna puntual\n",
    "\n",
    "promedio = df_1['columna_4'].mean()\n",
    "desvio = df_1['columna_4'].std()\n",
    "perc = df_1['columna_4'].quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "print(f'Promedio:\\n{round(promedio, 4)}', f'Desvío:\\n{round(desvio, 4)}', 'Percentiles:', perc, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# máximo, mínimo, mediana\n",
    "\n",
    "maximo = df_2.max()\n",
    "minimo = df_2.min()\n",
    "med  = df_2.median()\n",
    "\n",
    "print(f'Máximo:\\n{maximo}', f'Mínimo:\\n{minimo}', f'Moda:\\n{med}', sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# máximo, mínimo, moda, n-mayores y n-menores\n",
    "\n",
    "maximo = df_1['columna_4'].max()\n",
    "minimo = df_1['columna_4'].min()\n",
    "med = df_1['columna_4'].median()\n",
    "n_mayores = df_1['columna_4'].nlargest(5)\n",
    "n_menores = df_1['columna_4'].nsmallest(5)\n",
    "\n",
    "print(f'Máximo: {maximo}', f'Mínimo: {minimo}', f'Moda: {med}', 'n_mayores:', n_mayores, 'n_menores', n_menores, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones lógicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame(np.random.randint(low=0, high=100, size=(100, 5)), columns=[f'columna_{i}' for i in range(5)])\n",
    "df_2 = pd.DataFrame(np.random.randint(low=20, high=150, size=(100, 2)), columns=[f'columna_{i}' for i in range(2)])\n",
    "k = 20\n",
    "\n",
    "# gt o ge\n",
    "print(df_1['columna_4'] > k)\n",
    "print(df_1['columna_4'].ge(k))\n",
    "print(df_1['columna_0'].ge(df_1['columna_2']))\n",
    "\n",
    "# lt o le\n",
    "print(df_2['columna_0'] < k)\n",
    "print(df_2['columna_0'].le(k))\n",
    "print(df_2['columna_1'].le(df_1['columna_3']))\n",
    "\n",
    "# eq o ne\n",
    "print(df_1['columna_4'] == k)\n",
    "print(df_1['columna_2'].ne(k))\n",
    "print(df_1['columna_3'].eq(df_1['columna_0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nos permite además aplicar la operación en cuestión a más de una columna\n",
    "\n",
    "display((df_1[['columna_2', 'columna_4']] > k).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permite múltiples condiciones\n",
    "print((df_2['columna_0'] > 0.75 * k) & (df_2['columna_0'] < 1.25 * k))\n",
    "\n",
    "# e incluso en múltiples columnas\n",
    "print((df_2['columna_0'] > 45) & (df_2['columna_1'] < 65))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrado y slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = {'Nombre': ['Mike', 'James', 'Tom', 'Sarah'],\n",
    "         'Edad': [25, 30, 22, 35],\n",
    "         'Ciudad': ['Nueva York', 'Chicago', 'Chicago', 'Nueva York']}\n",
    "\n",
    "df = pd.DataFrame(datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtrar por una sola condición\n",
    "\n",
    "# filtrar personas mayores de 25 años\n",
    "resultado_filtrado = df[df['Edad'] > 25]\n",
    "\n",
    "print(\"DataFrame original:\")\n",
    "display(df)\n",
    "\n",
    "print(\"\\nPersonas mayores de 25 años:\")\n",
    "display(resultado_filtrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtrar por varias condiciones\n",
    "\n",
    "# filtrar personas mayores de 25 años y que viven en Nueva York\n",
    "resultado_filtrado = df[(df['Edad'] > 25) & (df['Ciudad'] == 'Nueva York')]\n",
    "\n",
    "print(\"\\nPersonas mayores de 25 años que viven en Nueva York:\")\n",
    "display(resultado_filtrado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para aquellos que tienen edad igual a 22 y viven en Chicago, seleccionar solo la columna 'Nombre'\n",
    "resultado_slicing = df.loc[(df['Edad']==22) & (df['Ciudad']=='Chicago'), ['Nombre']]\n",
    "\n",
    "print(\"DataFrame original:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nColumnas 'Nombre' y 'Ciudad' seleccionadas:\")\n",
    "display(resultado_slicing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seleccionar las filas 1 a 2\n",
    "resultado_slicing = df.iloc[1:3]\n",
    "\n",
    "print(\"\\nFilas 1 a 2 seleccionadas:\")\n",
    "display(resultado_slicing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenar y Merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.column_stack([np.random.randint(low=0, high=5, size=100),np.random.randint(low=0, high=100, size=(100, 5))])\n",
    "data_ = np.column_stack([np.random.randint(low=0, high=5, size=100),np.random.uniform(low=0, high=100, size=(100, 5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame(data, columns=['columna_id']+[f'columna_{i}' for i in range(data.shape[1]-1)])\n",
    "df_2 = pd.DataFrame(data_, columns=['columna_id']+[f'columna_{i}' for i in range(data_.shape[1]-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenar\n",
    "\n",
    "* Existe la capacidad de *unir* dos o más ```pd.DataFrame``` por medio de la función ```pd.concat```\n",
    "* La operación de *unir* dichas series se conoce como **concatenar**\n",
    "* Puede ser realizada de dos formas:\n",
    "\n",
    "##### Verticalmente: \n",
    "\n",
    "```\n",
    "DataFrame 1\n",
    "```\n",
    "| Índice | Columna 1 |  Columna 2 |\n",
    "|--------|---------|--------------|\n",
    "| a      | 1       | 1            |\n",
    "| b      | 2       | 2            |\n",
    "| c      | 3       | 3            |\n",
    "```\n",
    "DataFrame 2\n",
    "```\n",
    "| Índice | Columna 1 |  Columna 2 |\n",
    "|--------|---------|--------------|\n",
    "| d      | 4       | 4            |\n",
    "| e      | 5       | 5            |\n",
    "\n",
    "```\n",
    "Columna Concatenada\n",
    "```\n",
    "| Índice | Columna 1 Concatenada | Columna 2 Concatenada |\n",
    "|--------|-------------------|--------------|\n",
    "| a      | 1       | 1            |\n",
    "| b      | 2       | 2            |\n",
    "| c      | 3       | 3            |\n",
    "| d      | 4       | 4            |\n",
    "| e      | 5       | 5            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df_1, df_2], axis = 0) # axis = 0 representa 'vertical' o 'columna'\n",
    "\n",
    "print('Shape df original:', df.shape)\n",
    "print('Shape df concatenado:', df_concat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Horizontalmente: \n",
    "\n",
    "```\n",
    "DataFrame 1\n",
    "```\n",
    "| Índice | Columna 1 |  Columna 2 |\n",
    "|--------|---------|--------------|\n",
    "| a      | 1       | 1            |\n",
    "| b      | 2       | 2            |\n",
    "| c      | 3       | 3            |\n",
    "```\n",
    "DataFrame 2\n",
    "```\n",
    "| Índice | Columna 3 |  Columna 4 |\n",
    "|--------|---------|--------------|\n",
    "| a      | 4       | 4            |\n",
    "| b      | 5       | 5            |\n",
    "\n",
    "```\n",
    "Columna Concatenada\n",
    "```\n",
    "| Índice | Columna 1 Concatenada | Columna 2 Concatenada | Columna 3 Concatenada | Columna 4 Concatenada |\n",
    "|--------|-------------------|--------------|---------|---------|\n",
    "| a      | 1       | 1            | 4       | 5      |\n",
    "| b      | 2       | 2            | 4       | 5      |\n",
    "| c      | 3       | 3            | NaN       | Nan      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df_1, df_2.iloc[:, 1:]], axis = 1) # axis = 1 representa 'horizontal' o 'fila'\n",
    "\n",
    "print('Shape df original:', df.shape)\n",
    "print('Shape df concatenado:', df_concat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge\n",
    "\n",
    "En Pandas, la función ```merge``` se utiliza para combinar dos o más DataFrames basándose en una o más columnas comunes. Esta operación es similar a los joins en SQL y proporciona una forma poderosa de combinar datos en función de claves específicas.\n",
    "\n",
    "**Sintaxis básica**\n",
    "\n",
    "```python\n",
    "\n",
    "pd.merge(df_1, --> dataframe en posición izquierda\n",
    "         df_2, --> dataframe en posición derecha\n",
    "         how=str, --> forma en la que se realizará el merge\n",
    "         on=str, --> nombre de la/s columna/s que se usará/n para hacer el merge\n",
    "         ...\n",
    "         # si se quiere unir por medio de columnas con nombres diferentes\n",
    "         left_on=str, --> nombre de la/s columna/s que se usará/n para hacer el merge por izquierda\n",
    "         right_on=str,--> nombre de la/s columna/s que se usará/n para hacer el merge por derecha\n",
    "         ...\n",
    "         # si los dataframes que se quieren unir tiene los mismos nombres de columnas\n",
    "         suffixes=(\"_x\", \"_y\") --> se le adiciona a cada columna repetida para diferenciarlas\n",
    "         )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame de base\n",
    "df1 = pd.DataFrame({'clave': ['A', 'B', 'C'], 'valor1': [1, 2, 3]})\n",
    "df2 = pd.DataFrame({'clave': ['B', 'C', 'D'], 'valor2': [4, 5, 6]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Inner Join**: devuelve solo las filas que tienen valores coincidentes en ambas tablas. Es la operación predeterminada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Join\n",
    "resultado_inner = pd.merge(df1, df2, on='clave', how='inner')\n",
    "display(resultado_inner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Left Join**: devuelve todas las filas del DataFrame izquierdo y las filas coincidentes del DataFrame derecho. Si no hay coincidencias, se llenarán con valores NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left Join\n",
    "resultado_left = pd.merge(df1, df2, on='clave', how='left')\n",
    "display(resultado_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Right Join**: devuelve todas las filas del DataFrame derecho y las filas coincidentes del DataFrame izquierdo. Si no hay coincidencias, se llenarán con valores NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right Join\n",
    "resultado_right = pd.merge(df1, df2, on='clave', how='right')\n",
    "display(resultado_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Outer Join (Full Outer Join)**: devuelve todas las filas de ambos DataFrames, llenando con valores NaN donde no hay coincidencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer Join\n",
    "resultado_outer = pd.merge(df1, df2, on='clave', how='outer')\n",
    "display(resultado_outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejemplo usando 'left_on' y 'right_on'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'key_left': ['A', 'B', 'C'], 'value_left': [1, 2, 3]})\n",
    "df2 = pd.DataFrame({'key_right': ['B', 'C', 'D'], 'value_right': [4, 5, 6]})\n",
    "\n",
    "# merge usando left_on and right_on\n",
    "resultado_merge = pd.merge(df1, df2, left_on='key_left', right_on='key_right', how='inner')\n",
    "display(resultado_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejemplo usando 'suffixes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'clave': ['A', 'B', 'C'], 'valor': [1, 2, 3]})\n",
    "df4 = pd.DataFrame({'clave': ['B', 'C', 'D'], 'valor': [4, 5, 6]})\n",
    "\n",
    "# Merge usando suffixes\n",
    "resultado_suffixes = pd.merge(df3, df4, on='clave', how='left', suffixes=('_left', '_right'))\n",
    "display(resultado_suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llenado de datos faltantes\n",
    "\n",
    "Puede ocurrir que la fuente de datos a usar posean valores nulos (en otras palabras, posiciones en las cuales la serie no posee valor alguno). Para sobrepasar ese problema, se poseen diversas alternativas:\n",
    "* filtrar valores nulos\n",
    "* completar/computar dichas posiciones con determinados valor\n",
    "    * con valor arbitrario (```fillna(n)```)\n",
    "    * con valor estadístico de la serie (```fillna(n_stat)```)\n",
    "    * con valor posterior/anterior (```ffill()```, ```bfill()```)\n",
    "* completar con valores extraídos de otras columnas\n",
    "* completas con valores relativos a otras columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Auxliar ###\n",
    "\n",
    "data1 = np.random.choice(a=[np.nan, 10, 20 , 30, 40, 50], p=[0.1, 0.2, 0.2, 0.2, 0.15, 0.15], size=(1000, 2))\n",
    "\n",
    "data2= np.random.normal(loc=50, scale=15, size=(1000, 2))\n",
    "\n",
    "df = pd.DataFrame(np.column_stack([data1, data2]), columns=['a', 'b', 'c', 'd'])\n",
    "\n",
    "print('Shape df original:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. a- filtrar valores nulos\n",
    "\n",
    "df_dropna = df.dropna()\n",
    "print('Shape df SIN NULOS:', df_dropna.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. b- filtrar por los valores nulos de una o más columna/s\n",
    "\n",
    "df_filt = df.loc[(~df['b'].isna()) & (~df['a'].isna())]  # podría usar además el método \"notnull\"\n",
    "print('Shape df filtrando por columna:', df_dropna.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. completar con un valor específico\n",
    "\n",
    "df_fillna = df.fillna({\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3}) # indico que valor asignar a los valores nulos de cada columna\n",
    "\n",
    "null_idxs = df[df['a'].isna()].index[:5]\n",
    "\n",
    "display(df_fillna.iloc[null_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. completar con valor estadístico\n",
    "\n",
    "# calculo la media de cada columna\n",
    "means = df.mean()\n",
    "\n",
    "display(df.fillna(means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. completar con un valor extraído de otra columna\n",
    "\n",
    "# se completa con el valor que se encuentra en la misma ppsición para la columna 'b'\n",
    "df.loc[df['a'].isna(), 'a'] = df.loc[df['a'].isna(), 'b'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. completar con un valor relativo a otra/s columna/s\n",
    "\n",
    "value = np.mean((df['c'] * 2) / df['d'])\n",
    "\n",
    "df[['b']].fillna(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupación y operaciones agrupadas\n",
    "\n",
    "* [Pandas Groupby - Documentación](https://pandas.pydata.org/docs/user_guide/groupby.html)\n",
    "* [Pandas Groupby - GeekForGeeks](https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/)\n",
    "\n",
    "La agrupación y las operaciones agrupadas en Pandas son esenciales para realizar análisis y cálculos estadísticos sobre conjuntos de datos que contienen categorías o grupos específicos. La función principal para llevar a cabo estas operaciones es ```groupby```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Producto': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'A'],\n",
    "        'Region': ['Norte', 'Sur', 'Norte', 'Sur', 'Norte', 'Sur', 'Norte', 'Sur'],\n",
    "        'Ventas': [100, 150, 120, 80, 200, 180, 250, 220]}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por 'Producto' y 'Region'\n",
    "grupo_df = df.groupby(['Producto', 'Region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la suma y el promedio de ventas por grupo\n",
    "resultados_agrupados = grupo_df.agg({'Ventas': ['sum', 'mean']})\n",
    "\n",
    "display(resultados_agrupados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el máximo y mínimo de ventas por grupo\n",
    "resultados_agrupados = grupo_df.agg({'Ventas': ['max', 'min']})\n",
    "\n",
    "display(resultados_agrupados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar solo las filas donde las ventas superan el promedio del grupo\n",
    "filtrado_por_promedio = df[df['Ventas'] > grupo_df['Ventas'].transform('mean')]\n",
    "\n",
    "display(filtrado_por_promedio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar solo por 'Region'\n",
    "grupo_df = df.groupby('Region')\n",
    "\n",
    "# Obtener el producto más vendido y el total de valor por región\n",
    "resultados_agrupados = grupo_df.agg({'Producto': lambda x: x.mode().iloc[0],\n",
    "                                     'Ventas': 'sum'})\n",
    "\n",
    "display(resultados_agrupados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura y escritura de datos\n",
    "\n",
    "* [Documentación Pandas](https://pandas.pydata.org/docs/getting_started/intro_tutorials/02_read_write.html)\n",
    "\n",
    "Pandas permite realiza realziar operaciones de **lectura** y **escritura** de **datos en formato tabular**.\n",
    "\n",
    "Los principales formatos (que son de interés para este curso) son:\n",
    "\n",
    "\n",
    "* ```csv``` (Comma-Separated Values):\n",
    "\n",
    "    * Descripción: Formato de archivo simple que utiliza comas para separar valores. Es legible por humanos y ampliamente utilizado para almacenar datos tabulares.\n",
    "    * Uso común: Intercambio de datos tabulares entre aplicaciones, almacenamiento de datos en bruto.\n",
    "    * Ejemplo de extensión de archivo: *.csv*\n",
    "\n",
    "* ```parquet```:\n",
    "\n",
    "    * Descripción: Formato de almacenamiento de datos de columna eficiente para análisis de Big Data. Diseñado para ser eficiente en términos de espacio y velocidad de lectura/escritura.\n",
    "    * Uso común: Almacenamiento de grandes conjuntos de datos, especialmente en entornos de big data.\n",
    "    * Ejemplo de extensión de archivo: *.parquet*\n",
    "\n",
    "* ```pickle```:\n",
    "\n",
    "    * Descripción: Formato de serialización en Python. Permite convertir objetos Python en una secuencia de bytes y viceversa.\n",
    "    * Uso común: Almacenamiento y carga de objetos complejos de Python, persistencia de modelos de aprendizaje automático.\n",
    "    * Ejemplo de extensión de archivo: *.pkl*, *.pickle*\n",
    "\n",
    "* ```url``` (Uniform Resource Locator):\n",
    "\n",
    "    * Descripción: No es un formato de archivo per se, sino una dirección única que identifica una ubicación en la web. Puede apuntar a archivos de diferentes formatos, y la lectura/escritura depende del formato específico.\n",
    "    * Uso común: Acceso a recursos en línea, lectura/escritura de datos directamente desde/hacia la web.\n",
    "    * Ejemplo: *https://ejemplo.com/datos.csv*\n",
    "\n",
    "<br>\n",
    "\n",
    "* **Métodos/funciones principales**\n",
    "\n",
    "    * **Lectura**: ```pd.read_*``` (es una *función* de Pandas)\n",
    "    * **Escritura**: ```df.to_*``` (es un *método* de la clase pd.DataFrame)\n",
    "\n",
    "*El ```*``` se reemplaza por el tipo de archivo que se desea.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Genero data auxiliar ###\n",
    "n_cols = 5\n",
    "df = pd.DataFrame(np.random.randint(low=0, high=1000, size=(100, n_cols)), columns=[f'columna_{i}' for i in range(n_cols)])\n",
    "\n",
    "### Creo una carpeta donde guardar los datos que se generen en este notebook ###\n",
    "\n",
    "import os\n",
    "\n",
    "path = \"../data/Modulo-0\" # en este path se almacenarán los datos\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "print('Carpetas generadas ok!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  **csv**\n",
    "\n",
    "##### ```to_csv```\n",
    "***Parámetros:***\n",
    "* ```path_or_buf``` (str or file-like): Ruta del archivo CSV o un objeto de archivo para escribir.\n",
    "* ```sep``` (str, default=','): Delimitador a utilizar en el archivo CSV para separar campos.\n",
    "* ```header``` (bool or list of str, default=True): Indica si escribir o no el encabezado. Si es una lista de strings, se utilizará como el encabezado.\n",
    "* ```index``` (bool, default=True): Indica si escribir o no el índice del DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'mi_archivo.csv'\n",
    "\n",
    "print(os.path.join(path, file)) # me devuelve la unión de la ubicación de la carpeta más el nombre del archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escritura\n",
    "\n",
    "df.to_csv(os.path.join(path, file), sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ```read_csv```\n",
    "\n",
    "***Parámetros:***\n",
    "\n",
    "* ```filepath_or_buffer``` (str o objeto de tipo archivo): Ruta del archivo CSV o un objeto de archivo que se va a leer.\n",
    "\n",
    "* ```sep``` (str, default=','): Delimitador utilizado en el archivo CSV para separar campos.\n",
    "\n",
    "* ```header``` (int, list of int, default='infer'): Número de filas que se deben usar como encabezado o una lista de números de fila para utilizar como encabezados. Si es 'infer', intentará inferir automáticamente el encabezado.\n",
    "\n",
    "* ```names``` (array-like, optional): Lista de nombres para asignar a las columnas.\n",
    "\n",
    "* ```index_col``` (int, str, sequence[int/str], or False, default=None): Columna(s) que se deben utilizar como índice del DataFrame. Puede ser el número de la columna o el nombre de la columna.\n",
    "\n",
    "* ```usecols``` (list-like or callable, optional): Columnas a seleccionar para cargar. Si es una función, se aplicará a cada línea con el índice devuelto por index_col.\n",
    "\n",
    "* ```nrows``` (int, default=None): Cantidad de filas a cargar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lectura\n",
    "\n",
    "read_df = pd.read_csv(os.path.join(path, file), sep=';')\n",
    "\n",
    "print('Shape read_df:', read_df.shape)\n",
    "read_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_df = pd.read_csv(os.path.join(path, file), sep=';', usecols=['columna_1', 'columna_3'], nrows=50)\n",
    "\n",
    "print('Shape read_df:', read_df.shape)\n",
    "read_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **parquet**\n",
    "\n",
    "```to_parquet```\n",
    "\n",
    "***Parámetros***:\n",
    "\n",
    "* ```path``` (str): Ruta del archivo Parquet.\n",
    "\n",
    "* ```engine``` (str, optional): Motor de escritura. Puede ser 'auto', 'pyarrow', o 'fastparquet'. El valor 'auto' intenta utilizar 'pyarrow', pero cambia a 'fastparquet' si no está disponible.\n",
    "\n",
    "* ```compression``` (str, optional): Método de compresión a utilizar. Puede ser 'snappy', 'gzip', o 'brotli'.\n",
    "\n",
    "* ```index``` (bool, default=True): Indica si escribir o no el índice del DataFrame.\n",
    "\n",
    "* ```partition_cols``` (list, optional): Lista de columnas por las que particionar el DataFrame al escribir en formato Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'mi_archivo.parquet'\n",
    "\n",
    "print(os.path.join(path, file)) # me devuelve la unión de la ubicación de la carpeta más el nombre del archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escritura\n",
    "\n",
    "df.to_parquet(os.path.join(path, file), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```read_parquet```\n",
    "\n",
    "***Parámetros***\n",
    "* ```path``` (str): Ruta del archivo Parquet.\n",
    "\n",
    "* ```engine``` (str, optional): Motor de lectura. Puede ser 'auto', 'pyarrow', o 'fastparquet'. El valor 'auto' intenta utilizar 'pyarrow', pero cambia a 'fastparquet' si no está disponible.\n",
    "\n",
    "* ```columns``` (list, optional): Lista de columnas a leer desde el archivo Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lectura\n",
    "\n",
    "read_df = pd.read_parquet(os.path.join(path, file))\n",
    "\n",
    "print('Shape read_df:', read_df.shape)\n",
    "read_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **pickle**\n",
    "\n",
    "```to_pickle```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'mi_archivo.pkl'\n",
    "\n",
    "print(os.path.join(path, file)) # me devuelve la unión de la ubicación de la carpeta más el nombre del archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escritura\n",
    "\n",
    "df.to_pickle(os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```read_pickle```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_df = pd.read_pickle(os.path.join(path, file))\n",
    "\n",
    "print('Shape read_df:', read_df.shape)\n",
    "read_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **URL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link o url del archivo\n",
    "url = \"https://gist.githubusercontent.com/bobbyhadz/9061dd50a9c0d9628592b156326251ff/raw/381229ffc3a72c04066397c948cf386e10c98bee/employees.csv\"\n",
    "\n",
    "# se usa el mismo read_csv ya que el formato de 'csv', lo que cambia es la ubicación del archivo a cargar, que en este caso es una url\n",
    "data = pd.read_csv(url, sep=',', encoding='utf-8')\n",
    "\n",
    "display(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
